% @Author: Kevin Kamm
% @Date:   2024-05-31 08:51:32
% @Last Modified by:   Kevin Kamm
% @Last Modified time: 2024-09-10 09:46:48
\documentclass[%
a4paper,							
11pt,								
bibliography=totoc,						
abstracton=true					
]
{scrartcl}
\usepackage[pagewise]{lineno}
% \linenumbers % Comment out for final version
\usepackage[a4paper,left=3.0cm,right=2.5cm, top=2.5cm, bottom=3.0cm]{geometry}
\usepackage[headsepline=.4pt,footsepline=.4pt,automark,autooneside=false]{scrlayer-scrpage}
\pagestyle{scrheadings} 
\automark[subsection]{section} 
\automark*[subsubsection]{subsection}
\ihead{\scriptsize\rightmark}\chead{ }
\cfoot{\pagemark}
\usepackage{subcaption}

\usepackage[swedish,english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern} %latin modern
\usepackage{setspace} % 1.5-line spacing
\usepackage{csquotes} % provides \enquote
\usepackage{epigraph} % provides \epigraph
\setlength\epigraphwidth{.6\textwidth}
\setlength\epigraphrule{0pt}
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}
\interfootnotelinepenalty=10000 % avoid footnote page breaks
\usepackage{abstract}

\usepackage{calc}
\usepackage{xspace} % provides \xspace

% Math symbols & environments
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm,thmtools}
\usepackage{mathtools}
\usepackage{bbm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% Figures & color
\usepackage{graphicx} 
\usepackage{float}
\usepackage[section]{placeins} % float barrier after sections
\usepackage{pdflscape} % provides \includepdf
\usepackage[format=plain]{caption}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usepackage{tikz}
\usetikzlibrary{calc,positioning,arrows.meta}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

% Code Highlighting & Todo Notes
\usepackage{minted}
\usepackage{todonotes} % provides \todo

% Tables & Lists
\usepackage{paralist} % provides compact environments
\usepackage{tabularx} % provides tabularx
\usepackage{booktabs} % provides \toprule
\usepackage{multirow}
\usepackage{multicol}
\usepackage{diagbox} % provides \diagbox


\usepackage{subcaption}  % in your preamble

\usepackage{graphicx}
\usepackage{subcaption}



\usepackage[
	backend=bibtex,
	style=authoryear-comp,
	maxbibnames=9,
	maxcitenames=2,
	dashed=false,
	natbib=true,
	sortcites=true,
	block=space
]{biblatex}
\renewcommand*{\mkbibnamefamily}[1]{\textsc{#1}}
\renewcommand*{\mkbibnamegiven}[1]{\textsc{#1}}
\renewcommand*{\finalnamedelim}{\ \bibstring{and}\ }
\renewcommand*{\bibfont}{\footnotesize}
\addbibresource{literature.bib} 

\usepackage{hyperref}
\usepackage{cleveref} % provides \Cref
\hypersetup{
    colorlinks,
    linkcolor={red},
    citecolor={blue},
    urlcolor={blue}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% URL fix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{url}
\def\UrlBreaks{\do\/\do-\do\&\do.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Macros
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\1}{\mathbbm{1}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\D}{\mathrm{Dom \;}}
\DeclareMathOperator*{\argmin}{argmin}


%% Verbatim examples
\SaveVerb{python}=(Intel-)Python 3.10=
\newcommand{\python}{\protect\UseVerb{python}\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossary
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{glossaries}
\newacronym{PDF}{PDF}{Probability Density Function}
\newacronym{CDF}{CDF}{Cumulative Distribution Function}
\newacronym{NN}{NN}{Neural Network}
\newacronym{NC}{NC}{Neural Copula}
\newacronym{PIT}{PIT}{Probability Integral Transform}
\newacronym{GBM}{GBM}{Geometric Brownian Motion}
\newacronym{MLE}{MLE}{Maximum Likelihood Estimation}
\newacronym{CSD}{CSD}{Central Securities Depository}
\newacronym{SPAN}{SPAN}{Standard Portfolio Analysis of Risk}
\newacronym{VaR}{VaR}{Value at Risk}
\newacronym{MC}{MC}{Monte Carlo}
\newacronym{SDE}{SDE}{Stochastic Differential Equation}
\newacronym{ITM}{ITM}{Inverse Transform Method}
\newacronym{MAE}{MAE}{Mean absolute error}
\newacronym{ES}{ES}{Expected Shortfall}


\makeglossaries
\newglossarystyle{mylist}{%
  \setglossarystyle{list}% base this style on the list style
  \renewcommand*{\glossentry}[2]{%
    \item[\glsentryitem{##1}\glstarget{##1}{\glossentryname{##1}}]%
\glossentrydesc{##1}\glspostdescription\space}%
}
\setglossarystyle{mylist}
\renewcommand*{\glstextformat}[1]{\color{black}#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Color comment boxes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xcolor}
\usepackage{tcolorbox}

% Define a custom command for general instructions
\newtcolorbox{generalinstructions}{
    colback=orange!90, % More saturated orange background
    colframe=black, % Stronger orange border
    boxrule=1.2pt, % Slightly thicker border for sharpness
    arc=2pt, % Less rounded corners for a sharper look
    width=\linewidth, % Full width box
    before skip=10pt, % Space before the box
    after skip=10pt, % Space after the box
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Begin Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\onehalfspacing
\pagestyle{plain}
\pagenumbering{roman}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title Page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
    \begin{center}
    \begin{tikzpicture}[overlay,remember picture,align=center,anchor=north]
        \path let \p1 = (current page.north) in coordinate (pageNorth) at (0,\y1);
        \path let \p1 = ($(current page.north)!0.5!(current page.south)$) in coordinate (pageCenter) at (0,\y1);
        \node[align=center,anchor=north, yshift=-1cm] (LogoUmU) at (pageNorth) {\includegraphics{Figures/LogoUmU.png}};
        \node[align=center,anchor=north, below = of LogoUmU.south] (BackgroundUmu) {\includegraphics[width=.9\paperwidth]{Figures/Titlepage.jpg}};
        \node[anchor=south west,align=left] at (BackgroundUmu.south west) {\color{white}
            \begin{tabular}{lll}
                Supervisors & Kevin Kamm & Umeå University\\
                            & Victor Jonsson & Vermiculus Financial Technology\\
                Examiner & Christian Ewald & Umeå University
            \end{tabular}
        };
    \end{tikzpicture}
    \end{center}
\vspace*{5cm}
\begin{center}
    {%  
        \color{white}
        \noindent
        \Huge
        \bfseries 
        \sffamily
        Johans Thesis
    }\\[1em]
    {%
        \color{white}
        \noindent
        \Large
        \bfseries 
        \sffamily
        Subtitle of Thesis
    }%
\end{center}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
%\footnotetext[1]{Department of Mathematics and Statistics, Umeå University, Sweden}
\footnotetext[2]{johe6227@student.umu.se}
%\footnotetext[3]{E-mail: name.surname@umu.se}

\begin{center}
    \color{white}
    \large
    Johan Herbert%
    %\footnotemark[1]{}%\textsuperscript{,}%
    \footnotemark[2]{}
    \hspace{2em}
    % Name Surname%
    % \footnotemark[1]{}\textsuperscript{,}%
    % \footnotemark[3]{}
    \hspace{2em}
\end{center}
\begin{center}
    \color{white}
    \large
    Examination Date: \today
\end{center}
\vfill
\begin{center}
    Master thesis, 30 ECTS\\
    M.Sc in Industrial Engineering and Management, 300 ECTS\\
    Spring term 2025
\end{center}


\renewcommand{\thefootnote}{\arabic{footnote}}


\thispagestyle{empty}
\end{titlepage}
\pagenumbering{Roman}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstracts
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
    % This is the english abstract.
    % This thesis investigates the use of copulas for modeling dependency structures between financial asset returns, with a specific focus on a recently introduced method known as the \gls{NC}. The work is carried out in collaboration with Vermiculus Financial Technology and aims to enhance risk modeling in clearing systems. Traditional copulas are compared to the \gls{NC} in terms of performance, adaptability, and applicability to different dependency structures. The study involves the development of an alternative method for fitting marginal distributions and a practical sampling approach for the \gls{NC}. Simulated data is used to evaluate the methods under controlled conditions. The results show that the \gls{NC} cannot provide competitive performance compared to other copulas in capturing complex dependencies. This work contributes to the understanding of when and how to use neural copulas in financial risk modeling.

    This thesis explores the use of neural networks for estimating copula functions, with the goal of modeling the joint distribution of financial time series. Specifically, it evaluates a recently proposed neural copula (NC) approach that incorporates copula properties directly into the neural network's loss function. Three experiments were conducted to assess the model’s performance. The first confirmed the adequacy of the NC's marginal distribution modeling. The second identified optimal hyperparameters for stable training of the NC. The third compared the NC to traditional copula methods using synthetic data, showing that the NC did not outperform classical methods in the tested scenarios. Contributions of this work include a practical sampling method for the NC, an investigation of alternative marginal fitting techniques, and a critical comparison across copula models. The thesis concludes with reflections on the limitations and learning outcomes, and proposes several directions for future research, such as improving numerical stability, extending the NC to high-dimensional settings, and integrating copulas into dynamic volatility models.
\end{abstract}

\newpage
\renewcommand{\abstractname}{Sammanfattning}
\begin{abstract}
    % Det är svenska sammanfattning.
    % Denna masteruppsats undersöker användningen av copulor för att modellera beroendestrukturer mellan finansiella tillgångars avkastningar, med särskilt fokus på en nyligen introducerad metod kallad \gls{NC}. Arbetet har genomförts i samarbete med Vermiculus Financial Technology och syftar till att förbättra riskmodellering i clearingsystem. Traditionella copulor jämförs med \gls{NC} avseende prestanda, anpassningsförmåga och tillämplighet på olika typer av beroenden. Studien omfattar utvecklingen av en alternativ metod för att anpassa marginalfördelningar samt ett praktiskt tillvägagångssätt för slumptalsgenerering från \gls{NC}. Simulerad data används för att utvärdera metoderna under kontrollerade förhållanden. Resultaten visar att \gls{NC} inte producerar bättre resultat än övriga traditionella metoder vid modellering av komplexa beroenden, även om dess effektivitet är beroende av datakaraktäristik och korrekt träning. Uppsatsen bidrar till en ökad förståelse för när och hur neural copula kan användas inom finansiell riskmodellering.

    Denna avhandling undersöker användningen av neurala nätverk för att uppskatta kopulafunktioner, med målet att modellera den gemensamma fördelningen av finansiella tidsserier. Särskilt utvärderas en nyligen föreslagen metod med neurala kopulor (NC) som integrerar kopulaegenskaper direkt i det neurala nätverkets förlustfunktion. Tre experiment genomfördes för att bedöma modellens prestanda. Det första bekräftade att NC:n på ett tillfredsställande sätt modellerar de marginala fördelningarna. Det andra identifierade optimala hyperparametrar för stabil träning av NC:n. Det tredje jämförde NC:n med traditionella kopulametoder med syntetiska data och visade att NC:n inte överträffade klassiska metoder i de testade scenarierna. Avhandlingens bidrag inkluderar en praktisk samplingsmetod för NC:n, en undersökning av alternativa tekniker för anpassning av marginalfördelningar samt en kritisk jämförelse mellan olika kopulamodeller. Avslutningsvis diskuteras begränsningar och lärdomar från arbetet, samt föreslås flera riktningar för framtida forskning, såsom att förbättra numerisk stabilitet, utöka NC:n till högdimensionella tillämpningar och integrera kopulor i dynamiska volatilitetmodeller.

\end{abstract}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
I would like to express my sincere gratitude to my supervisor, Kevin Kamm at Umeå University, for his unwavering support, insightful guidance, and enthusiasm throughout this thesis. His expertise and mentorship have been instrumental in shaping this work and continually challenging me to think critically and push boundaries.

I am also deeply thankful to Victor Jonsson and Vermiculus Financial Technology for the opportunity to collaborate on this project. Their warm welcome, encouragement, and practical support created a truly rewarding and engaging experience.

Lastly, I would like to thank my family and friends for their continued support, patience, and encouragement throughout this journey.  

Johan Herbert\\
Stockholm, Sweden\\
\today 

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossaries & Table of Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \printglossary
\clearpage
\newpage
{\hypersetup{linkcolor=black}
\tableofcontents
}
\newpage
\pagenumbering{arabic}
\pagestyle{scrheadings}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}\label{sec:Introduction}
\input{1Introduction/Introduction}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Theory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theory}\label{sec:theory}
\input{3Theory/Theory}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Methodology 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}\label{sec:Method}
\input{4Method/Method}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Results and Discussion 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}\label{sec:Results}
\input{5ResultsDiscussion/ResultsAndDiscussion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:Conclusion}
\input{6Conclusion/Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \nocite{*}
\printbibliography


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Other Copulas
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Other copulas}\label{sec:OtherCopulas}
This section introduces some of the most commonly used copulas. Some copulas are defined through known statistical distributions such as the Gaussian copula and the Student's t copula. Another type of copulas is the family of Archimedean copulas, of which the Clayton copula is an example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Gaussian Copula
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gaussian Copula}\label{sec:GaussianCopula}
A commonly used copula is the Gaussian copula which is derived from the centered multivariate normal distribution. 

Let $\boldsymbol{\Phi}_\Sigma(x_1,x_2)$ be the bivariate normal distribution \gls{CDF} with correlation matrix $\Sigma$. Let $\Phi(x)$ be the univariate normal \gls{CDF}. Then the Gaussian copula is defined, as in \citet[p.~112]{Umberto2004copulaMethods}  by 
\begin{align*}
    C_\Sigma(u_1,u_2) = \boldsymbol{\Phi}_\Sigma(\Phi^{-1}(u_1),\Phi^{-1}(u_2)).
\end{align*}

In the above expression $\Sigma$ is the correlation matrix in two dimensions 
\begin{align*}
    \Sigma = 
    \begin{bmatrix}
            1 & \rho_{1,2} \\
            \rho_{1,2} & 1
    \end{bmatrix},
\end{align*}
so in the sequel when $\rho$ is used it refers to $\rho_{1,2}$ in the correlation matrix.

The copula function, which is a \gls{CDF} can be expressed on integral form as done by \citet[p.~112]{Umberto2004copulaMethods}
\begin{align*}
     C_{\Sigma} (u_1,u_2)
    = \int_{-\infty}^{\Phi^{-1}(u_1)}\int_{-\infty}^{\Phi^{-1}(u_2)}
    \frac{1}{2\pi\sqrt{1-\rho^2}} \mathrm{exp}\left\{ - \frac{s^2-2\rho st+t^2}{2(1-\rho^2)}   \right\} dsdt.
\end{align*} 

The \gls{PDF} of the Gaussian copula function can be shown to be 
\begin{align*}
     c_{\Sigma} (u_1,u_2)
    = \frac{1}{\sqrt{1-\rho^2}} \mathrm{exp}\left\{  \frac{-2\rho^2\Phi^{-1}(u_1)^2  +2\rho \Phi^{-1}(u_1)\Phi^{-1}(u_2) -2\rho^2\Phi^{-1}(u_2)^2}{2(1-\rho^2)}   \right\},
\end{align*}
as stated in \citet[p.267]{Alexander2008}.



%\textbf{Fitting}(alternative source: \Citet[pp.~281-283]{Alexander2008})
To fit the Gaussian copula to data we need to compute the correlation matrix $\Sigma$. This is done by first estimating the sample covariance matrix and then normalizing it to obtain the correlation matrix\footnote{See "SAS/ETS\textsuperscript{\textregistered} 13.2 User's Guide" p.523, \textit{SAS Institute Inc}, Published: 2014, \url{https://support.sas.com/documentation/onlinedoc/ets/132/copula.pdf}, Last Accessed: 2025-04-24.}.%\Citet[p.~523]{SAS2014}. 

%\textbf{Sampling}(alternative source: \Citet[p.~19]{Schmidt2006})
To sample from a Gaussian copula we can first generate a sample from the multivariate normal distribution with the desired correlation matrix $\Sigma$. This is done by multiplying a standard normal random vector with the Cholesky decomposition of the correlation matrix. The Cholesky decomposition can be thought of as a matrix square root of the correlation matrix. The resulting sample of correlated random numbers is then transformed to uniform random numbers using the \gls{PIT}. This is done by applying the univariate normal \gls{CDF} to each of the elements in the sample. The resulting sample is then a sample from the Gaussian copula\footnotemark[\value{footnote}]. %\footnote{\value{\footnote}} %\footnote{See \url{https://support.sas.com/documentation/onlinedoc/ets/132/copula.pdf}. \textit{SAS/ETS\textsuperscript{\textregistered} 13.2 User's Guide}, p.523, SAS Institute Inc., Cary, NC, 2014. Last Accessed: 2025-04-24.}.%\Citet[p.~523]{SAS2014}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Students Copula
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Students t Copula}\label{sec:StudentsCopula} 
The Student's $t$ copula can be defined similarly to the Gaussian copula above. We denote the Student's $t$ copula by 
\begin{align*}
    C_{\nu,\Sigma}(u_1,u_2) = \boldsymbol{t}(t_\nu^{-1}(u_1),t_\nu^{-1}(u_1)),
\end{align*}
as done by \Citet[p.~268]{Alexander2008}, where $\nu >0$ is the degrees of freedom, $\boldsymbol{t}$ is the multivariate Student's $t$ distribution, $t$ is the univariate Student's $t$ distribution, and $\Sigma$ is the correlation matrix matrix.

The Student's $t$ copula \gls{CDF} is 
\begin{align*}
     C_{\nu,\Sigma}^t (u_1,u_2)
    = \int_{-\infty}^{t_\nu^{-1(u_1)}}\int_{-\infty}^{t_\nu^{-1(u_2)}}
    \frac{1}{2\pi\sqrt{1-\rho^2}} \left\{  1+ \frac{s^2-2\rho st+t^2}{\nu(1-\rho^2)}   \right\} dsdt,
\end{align*}
as defined by \citet[p.~116]{Umberto2004copulaMethods}. The corresponding copula \gls{CDF} is, by the same logic as for the Gaussian case, the integrand in the expression above times the two partial derivatives of the marginal distributions. The resulting expression for the copula \gls{PDF} $c_{\nu,\rho}$ is therefore 
\begin{align*}
    c_{\nu,\Sigma}(u_1,u_2) = \frac{\Gamma(\frac{\nu+2}{2})\Gamma(\frac{\nu}{2})\prod_{j=1}^2\left( 1+ \frac{t_\nu^{-1}(u_j)^2}{\nu}\right)^{\frac{\nu+2}{2}} } {\sqrt{\rho} \;\Gamma(\frac{\nu+1}{2})^{2}\left( 1+ \frac{t_\nu^{-1}(u_1)^2 + t_\nu^{-1}(u_2)^2 -2\rho t_\nu^{-1}(u_1) t_\nu^{-1}(u_2) }{\nu(1-\rho^2)}\right)},
\end{align*}
as stated by \Citet[p.~117]{Umberto2004copulaMethods}. In the above expression $\Gamma$ is the Euler gamma function.

%\textbf{Fitting} 
To fit the Student's $t$ copula to data we need to compute the correlation matrix $\Sigma$ and the degrees of freedom $\nu$. It is common to first estimate the correlation matrix and then estimate the degrees of freedom using \gls{MLE} \Citet[p.~283]{Alexander2008}. In this thesis we will use the Pearson correlation to estimate the correlation matrix. 

%\textbf{Sampling} %(alternative source: \Citet[p.~19]{Schmidt2006})
Sampling from a Student's $t$ copula is similar to sampling from a Gaussian copula. First, we generate a sample from the multivariate Student's $t$ distribution with the desired correlation matrix $\Sigma$ and degrees of freedom $\nu$. This can be done by generating a sample $Z$ from a normal distribution with correlation matrix $\Sigma$ as described for the Gaussian copula. These normal random numbers can then be transformed into Student's $t$ distributed sample $X$ by the calculation $X = Z\sqrt{\frac{\nu}{s}} $, where s is a sample from a $\chi^2$ distribution with $\nu$ degrees of freedom. The resulting sample of correlated Student's $t$ random numbers is then transformed to uniform random numbers using the \gls{PIT}. This is done by applying the univariate Student's $t$ \gls{CDF} with $\nu$ degrees of freedom to each of the elements in the sample. The resulting sample is then a sample from the Student's $t$ copula\footnote{See "SAS/ETS\textsuperscript{\textregistered} 13.2 User's Guide" p.524, \textit{SAS Institute Inc}, Published: 2014, \url{https://support.sas.com/documentation/onlinedoc/ets/132/copula.pdf}, Last Accessed: 2025-04-24.}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Clayton Copula
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clayton Copula}\label{sec:ClaytonCopula}
Archimedian copulas have the general expression  
\begin{align*}
    C(u_1,u_2) = \Psi^{-1}(\Psi(u_1),\Psi(u_2)),
\end{align*}
for some generator function $\Psi$, as defined by \Citet[p.~150]{Umberto2004copulaMethods}.

The Clayton copula, which is one instance of an Archimedean copula, uses the generator function
\begin{align*}
    \Psi(u) &=u^{-\alpha}-1, 
\end{align*}
and its inverse
\begin{align*}
    \Psi^{-1}(x) &= (x+1)^{\frac{-1}{\alpha}},
\end{align*}
where $\alpha > 0$. 

This gives the bivariate Clayton copula \gls{CDF}
\begin{align*}
    C(u_1,u_2) = (u_1^{-\alpha} + u_2^{-\alpha}-1)^{\frac{-1}{\alpha}}.
\end{align*}
The Clayton copula \gls{PDF} is given by the expression
\begin{align*}
    c(u_1,u_2) = (\alpha+1)(u_1^{-\alpha}+u_2^{-\alpha}-1)^{-2- \frac{1}{\alpha}}u_1^{-\alpha -1} u_2^{-\alpha -1},
\end{align*}
as stated by \Citet[p.~272]{Alexander2008}. 

%\textbf{Fitting}
Fitting the Clayton copula to data is done by estimating the parameter $\alpha$. This is done by using \gls{MLE} with the copula \gls{PDF} as the terms in the sum of the log likelihood function. \gls{MLE} applied to copula fitting is described in \Citet[p.~2]{Choros2010}. 

%\textbf{Sampling}
To sample from a Clayton copula we can use conditional sampling as described in \Cref{sec:ConditionalSampling}. It is convenient to sample from because there is an analytical expression for the Copula from which the conditional copula can be derived. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Example of Correlation not working
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Correlation Underestimating Dependence }\label{sec:CorrelationUnderestimates}
\begin{example} 
    \textbf{Correlation underestimates the true dependence} 
    Let $X$ be a random variable such that, $X \sim N(0,1)$. Obviously, $X$ and $e^X$ are dependent. The covariance, $\mathrm{Cov}(X,e^X)$ is
    \begin{align*}
        \mathrm{Cov}(X,e^X) &= E \left[  (X-E\left[  X \right])(e^X-E\left[  e^X \right])  \right]\\
         &=  E \left[  X(e^X-E\left[  e^X \right]) \right]\\
         &= E \left[  Xe^X  \right] - E \left[  X E \left[  e^X \right] \right]\\
         &= E \left[  Xe^X  \right]\\
         &= \int_{-\infty}^\infty xe^x \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}} dx\\ %by definition of expectation for Cont. rand. var. 
         &=  \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} x  e^{x -\frac{x^2}{2}} dx \\
         &= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}} x  e^{-\frac{1}{2}(x-1)^2+\frac{1}{2} } dx \\
         &= \frac{e^\frac{1}{2}}{\sqrt{2\pi}}\int_{-\infty}^\infty x  e^{-\frac{1}{2}(x-1)^2 } dx\\
         & \mathrm{Let\;} u = x-1 \; \mathrm{then\;} x = u+1\\
         &= \frac{e^\frac{1}{2}}{\sqrt{2\pi}}\int_{-\infty}^\infty (u+1)e^{\frac{-u^2}{2}} du\\
         &= \frac{e^\frac{1}{2}}{\sqrt{2\pi}} \left( \int_{-\infty}^\infty ue^{\frac{-u^2}{2}}du +\int_{-\infty}^\infty e^{\frac{-u^2}{2}} du  \right) \\
         &= \frac{e^\frac{1}{2}}{\sqrt{2\pi}}\sqrt{2\pi}   \\
         &= e^{\frac{1}{2}}.
    \end{align*}
    If dividing with the standard deviations of $X$ and $e^X$ we get the correlation. To do this we need to calculate the variances of $X$ and $e^X$. We know that $\mathrm{Var}(X) = 1$ but need to calculate the variance of $e^X$. The variance of $e^X$ is calculated as
    \begin{align*}
        \mathrm{Var}(e^X) &= E[e^{2X}] - E[e^X]^2\\
        &= e^2 - e.\\
    \end{align*}
    The correlation is then calculated as
    \begin{align*}
        \mathrm{corr}(X,e^X) &= \frac{\mathrm{cov}(x,e^X)}{\sqrt{\mathrm{var}(x)} \sqrt{\mathrm{var}(e^X)}}\\
        & = \frac{e^{\frac{1}{2}}}{1\sqrt{e^2-e}}\\
        &=\sqrt{\frac{1}{e-1}} \approx 0.76. 
    \end{align*}
    If instead using the copula to calculate the correlation we can see that the correlation is not capturing the true dependence between $X$ and $e^X$. To see this we can use the \gls{PIT} to transform $X$ and $e^X$ into uniform random variables. If we let $F_X(X)=U_1$ be the normal data transformed to uniform random variables and $F_{e^X}(e^X)=U_2$ be the log-normal data transformed to uniform random variables we can see that 
    \begin{align*}
        F_{e^X}(e^X) &= P(e^X \leq e^x)\\
        &= P(\ln(e^X) \leq \ln (e^x))\\
        &=P(X\leq x) = U2 = U1.
    \end{align*}
    Since $U_1 = U_2$ we have 
    \begin{align*}
        P(U_1\leq u1, U_2\leq u2) = P(U_1\leq u1, U_1\leq u2) = \min(u_1,u_2).
    \end{align*}
    This is the upper copula given by the upper Fréchet-Hoeffding bound, which is the copula corresponding to maximum dependence. Hence, correlation underestimates the true dependence between the random variables given that maximum dependence under the correlation framework is one. This underestimation of the correlation is because of the assumption of a normal or students $t$ distribution which is violated with the lognormal variable in the example. This shows how copulas can capture dependence when correlation fails. 

    In \Cref{fig:ExamplePlots} some illustrations of the example are shown. The the true copula \gls{CDF} is shown in \Cref{fig:TrueCopulaExponential} this can be compared to the estimated copula using correlation in \Cref{fig:CorrelationEstimationExponential}. %\Cref{fig:exponentialDependenceScatterProb} shows the scatter plot of the data in the probability space illustating what maximum dependence looks like in probability space. 
    \Cref{fig:exponentialDependenceScatterRet} shows the scatter plot of the data in the return space. This illustrates why correlation is insufficient as a dependency measure when the underlying dependence is non-linear. 
    
    % \begin{generalinstructions}
    %     Would be nice to tie this to something in finance. Such as the relationship between stocks returns and derivative prices.

    %     This shows that estimating the correlation in a setting when the different marginals are not both normal can be misleading. This is a problem in finance where one often uses correlation to estimate the dependence between different assets. This is especially true when using correlation to estimate the dependence between stocks and derivatives.
    % \end{generalinstructions}
\end{example}


\section{Conditional sampling from copulas}\label{sec:ConditionalSampling} 
For some copulas samples can be generated using conditional sampling which works as follows \Citet[p.~41]{Nelsen2006}. First, we generate two independent uniform random numbers $z_1,z_2 \sim \mathrm{Unif}(0,1)$. 
The first random number is calculated without respect for the second random number hence we set $u_2 =1$. We want to solve
\begin{align*}
    C(u_1,u_2) =z_1,
\end{align*}
for $u_1$ independently of $u_2$. 

By using that the copula is a \gls{CDF} and can be written as a probability we can see that 
\begin{align*}
    C(u_1,u_2) = P(U_1\leq u_1, U_2\leq u_2) = P(U_1\leq u_1,U_2 \leq 1) = P(U_1\leq u_1).
\end{align*} 
Since $u_2 = 1$ no constraint is enforced on $U_2$. By the definition of a copula we have that 
\begin{align*}
    C(u_1,1) = u_1 = z_1.
\end{align*}
So we set the first random number as $u_1 = z_1$.

The second random number is obtained by computing the conditional copula \gls{CDF} given the first random number. This is done by using the conditional copula \gls{CDF} defined as
\begin{align*}
    C_{2|1}(u_2|u_1) = \frac{\partial C(u_1,u_2)}{\partial u_1}.
\end{align*}
To compute the second random number we need to solve the equation
\begin{align*}
    C_{2|1}(u_2|u_1) = z_2,
\end{align*}
for $u_2$. That is
\begin{align*}
    u_2 = C^{-1}_{2|1}(z_2|z_1).
\end{align*}
The pair $(u_1,u_2)$ is now a sample from the copula. The method can be repeated to generate more samples from the copula. The requirement for the copula to be able to use this method is that the copula has a strictly increasing conditional copula. If not, the solution to the equation might not be unique which will cause problems in the solving step. 

\todo{$C^{-1}$ is the quasi inverse since copula can be flat} 

\section{Acceptance rejection sampling}\label{sec:AcceptanceRejection} 
Acceptance rejection sampling is a method of sampling from any distribution as long as the density function is known \Citet[p.~47]{RobertCasella2004}. The method works by generating a sample from a proposal distribution and then accepting or rejecting the sample based on a threshold. The sampling method is defined below as done by \Citet[p.~50]{RobertCasella2004}.

\begin{definition}
    \textbf{Acceptance rejection sampling}\\
    Let $X \sim f(x)$ and let $g(x)$ be a density function such that $M g(x) \geq f(x)$ for some constant $M \geq 1$. To sample $X \sim f$, we can generate   
    \begin{align*}
        Y \sim g \; \mathrm{and} \; U|Y = y \sim \mathrm{Unif}(0,Mg(y)),      
    \end{align*}
    until $0 \leq u \leq f(y)$.
\end{definition}
$M$ is a constant that scales $g$ so that it is always larger than $f$. In general, the standard uniform distribution can always be used as $g$ but it is often more efficient to use a distribution that is similar to the target distribution. If the proposal distribution is the uniform distribution $M$ is the maximum value of $f$.  




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Maximum Likelihood Estimation}\label{sec:MLE}
Another important concept which will be used when fitting the different copulas is the \gls{MLE}. To illustrate what it is, we first need to define the likelihood function as done by \Citet[p.~122]{wasserman2010statistics}
\begin{definition}
    \textbf{Likelihood function}
    The likelihood function for a sample of observations $X_1,\dots,X_n$ being $\mathrm{IID}$ with \gls{PDF} $f(x;\theta)$ is defined by 
    \begin{align*}
        \mathcal{L}_n(\theta) = \prod_{i=1}^n f(X_i;\theta).
    \end{align*}
\end{definition}

Usually, the likelihood function becomes very small when the sample size increases. This is because the likelihood is often a value smaller than one, and a product of such values often goes to zero. Therefore, it is common to instead of using the likelihood function use the log likelihood function defined by 
\begin{align*}
    l_n(\theta) = \log(\mathcal{L}_n(\theta)).
\end{align*}
We can see that the logarithm makes the product sum into a regular sum such that 
\begin{align*}
    l_n(\theta) = \log \left( \prod_{i=1}^n f(X_i;\theta) \right)= \sum_{i = 1}^n \log( f(X_i;\theta)).
\end{align*}

We can now define the \gls{MLE} as done by \Citet[p.~122]{wasserman2010statistics}
\begin{definition}
    The \emph{maximum likelihood estimator}, denoted by $\hat\theta_n$, is the value of $\theta$ that maximizes $\mathcal{L}_n(\theta)$.
\end{definition}
Maximizing the likelihood function is equivalent to maximizing the log likelihood function, meaning that the parameter estimate is the same \Citet[p.~123]{wasserman2010statistics}. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Distance measure derivation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distance Measure Derivation}\label{sec:DistanceDerivation}
The Wasserstein distance is defined as in \Citet[p.~1032]{hallin2021}. Let $\mu$ and $\nu$ be two probability measures on $\mathbb{R}^2$ and let $d$ be some distance measure. The Wasserstein distance is defined as the infimum of the cost of moving the dirt from one distribution to another. The cost is defined as the average distance moved times the amount of dirt moved. The Wasserstein distance is then given by
\begin{align*}
    W_p(\mu,\nu) = \inf_{\gamma \in \Gamma(\mu,\nu)} \left( \int_{\mathbb{R}^2} d(x,y)^p d\gamma(x,y) \right)^{\frac{1}{p}}.
\end{align*}


It can be calculated between two equally sized sets of data points by
\begin{align*}
    W_p(\mu,\nu) =  \inf_{\pi} \left( \frac{1}{n} \sum_{i=1}^n \| X_i - Y_{\pi(i)}  \|^p \right)^{\frac{1}{p}},
\end{align*} 


where $d$ is replaced by the euclidean distance\footnote{See \url{https://www.stat.cmu.edu/~larry/=sml/Opt.pdf}. \textit{Optimal Transport and Wasserstein Distance} p.12. Larry Wasserman. Last Accessed 2025-05-15.}. This is the linear assignment of points between the two sets having the smallest distance between them. $\pi$ can be viewed as an operator that chooses the best assignment of points between the two sets. The distance is then the average distance between the points in the two sets. This problem has high cubic complexity and is therefore really inefficient when the number of points is large. This distance is illustrated in \Cref{fig:WassersteinDistance} where two datasets are displayed. The Wasserstein distance is the minimum distance of points moved that makes the two distributions identical. The transportation that results in the smallest distance is shown in the figure as the lines between the points. 
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{3Theory/pictures/WassersteinIllustrated.png}
    \caption{Illustration of the Wasserstein distance between two distributions. The distance is the average distance between the points in the two distributions.}
    \label{fig:WassersteinDistance}
\end{figure}


In one dimension the Wasswerstein distance can be written\footnote{See \url{https://www.stat.cmu.edu/~larry/=sml/Opt.pdf}. \textit{Optimal Transport and Wasserstein Distance} p.4. Larry Wasserman. Last Accessed 2025-05-15.} as 
\begin{align*}
    W_p(\mu,\nu) = \left(  \int_{0}^1 |F_\mu(q)^{-1} - F_\nu(q)^{-1}|^p dq \right)^{\frac{1}{p}}.
\end{align*}
From this we obtain through a change of variables that the Wasserstein distance can be written as
\begin{align*}
    W_p(\mu,\nu) = \left( \int_{\mathbb{R}} |F_\mu(x) - F_\nu(x)|^p dx \right)^{\frac{1}{p}}. 
\end{align*}

This motivates that the integral of the absolute or squared differences between two \gls{CDF}s can be used as a distance measure between the distributions. Note that the following can no longer be called the Wasserstein distance. In two dimensions we therefore define our own distance measure between distributions as
\begin{align*}
    \mathrm{dist}(\mu,\nu) = \left(\iint_{\mathbb{R}^2} |F_\mu(x,y) - F_\nu(x,y)|^p dx dy \right) ^{\frac{1}{p}},
\end{align*}
where $F_\mu$ and $F_\nu$ are the \gls{CDF}s of the two distributions. These distributions are estimated using kernel density estimation with the gaussian kernel over a grid of points. After normalizing the densities to create a distribution, this gives estimates of the \gls{PDF}s for each data set. If computing the cumulative sum of the approximate \gls{PDF}s in each dimension we get an approximated \gls{CDF} of the data. We denote the estimated \gls{CDF}s by $\hat F_\mu$ and $\hat F_\nu$. 

The distance is then approximated by numerical integration over the grid of points as
\begin{align*}
    \mathrm{dist}(\mu,\nu) = \left( \sum_{i=1}^n \sum_{j=1}^m |\hat F_\mu(x_i,y_j) - \hat F_\nu(x_i,y_j)|^p \Delta x_i \Delta y_j \right)^{\frac{1}{p}},
\end{align*}
where $\Delta x_i$ and $\Delta y_j$ are the step sizes in the $x$ and $y$ dimensions respectively. The distance is then calculated as the average distance between the two distributions. The density estimates and the corresponding CDFs are illustrated in \Cref{fig:ApproxWasserstein}. The estimated \gls{PDF}s are shown in \Cref{fig:density1} and \Cref{fig:density2}. The estimated \gls{CDF}s are shown in \Cref{fig:dist1} and \Cref{fig:dist2}. The difference between the two distributions is shown in \Cref{fig:diffDist1and2} and the distance approximation is then calculated as the average distance between the two distributions. That is, the volume under the surface in \Cref{fig:diffDist1and2}. 


\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{3Theory/pictures/densityDist2.png}
        \caption{Density estimation of the first distribution.}
        \label{fig:density1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{3Theory/pictures/densityDist1.png}
        \caption{Density estimation of the second distribution.}
        \label{fig:density2}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{3Theory/pictures/CDFDist2.png}
        \caption{Distribution function estimate of the first distribution.}
        \label{fig:dist1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{3Theory/pictures/CDFDist1.png}
        \caption{Distribution function estimate of the second distribution.}
        \label{fig:dist2}
    \end{subfigure}
     
    \vfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{3Theory/pictures/diffDist1and2.png}
        \caption{Difference between the two distributions.}
        \label{fig:diffDist1and2}
    \end{subfigure}
    \caption{Density and CDF plots used in the distance calculation.}
    \label{fig:ApproxWasserstein}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Additional portfolio plots
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Additional plots of generated data}\label{sec:CopulaResultsData}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port1Gauss.png}
        \subcaption*{Gaussian copula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port1Student.png}
        \subcaption*{Students $t$ copula}
    \end{minipage}
    \vfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port1Clayton.png}
        \subcaption*{Clayton copula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port1NC.png}
        \subcaption*{Neural copula}
    \end{minipage}
    \caption{Fitted neural copula surfaces for Gaussian $\rho=0$ portfolio.}
    \label{fig:GeneratedDataGaussian0}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port2Gauss.png}
        \subcaption*{Gaussian copula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port2Students.png}
        \subcaption*{Students $t$ copula}
    \end{minipage}
    \vfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port2Clayton.png}
        \subcaption*{Clayton copula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port2NC.png}
        \subcaption*{Neural copula}
    \end{minipage}
    \caption{Fitted neural copula surfaces for Gaussian $\rho=0.7$ portfolio.}
    \label{fig:GeneratedDataGaussian07}
\end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port3Gauss.png}
        \subcaption*{Gaussian copula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port3Students.png}
        \subcaption*{Students $t$ copula}
    \end{minipage}
    \vfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port3Clayton.png}
        \subcaption*{Clayton copula}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/Port3NC.png}
        \subcaption*{Neural copula}
    \end{minipage}
    \caption{Fitted neural copula surfaces for Students $\rho=-0.8$ , $\nu = 3$ portfolio. To not take up excessive space the figures are small. The reader is encouraged to zoom in on the figures to see the details.}
    \label{fig:GeneratedDataStudents}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Copula surfaces
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Copula surface plots}\label{sec:CopulaSurfacesPlots}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/NCPort1.png}
        \subcaption*{Gaussian $\rho=0$}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/NCPort2.png}
        \subcaption*{Gaussian $\rho=0.7$}
    \end{minipage}
    \vfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/NCPort3.png}
        \subcaption*{Students $\rho=-0.8$ , $\nu = 3$}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/NCPort4.png}
        \subcaption*{Clayton $\alpha=4$}
    \end{minipage}

    \caption{Copula surfaces for the different datasets. To not take up excessive space the figures are small. The reader is encouraged to zoom in on the figures to see the details.}
    \label{fig:NeuralCopulaSurface}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Copula gradients
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Copula gradient plots}\label{sec:CopulaGradientsPlots}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/GradPort1.png}
        \subcaption*{Gaussian $\rho=0.7$}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/GradPort2.png}
        \subcaption*{Gaussian $\rho=0.7$}
    \end{minipage}
    \vfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/GradPort3.png}
        \subcaption*{Students $\rho=-0.8$ , $\nu = 3$}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5ResultsDiscussion/pictures/PortfolioTest/GradPort4.png}
        \subcaption*{Clayton $\alpha=4$}
    \end{minipage}
    \caption{Copula density surfaces for the different datasets. To not take up excessive space the figures are small. The reader is encouraged to zoom in on the figures to see the details.}
    \label{fig:NeuralCopulaGradient}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Code}\label{sec:Code}
In this section, the code used in this thesis is provided. Given the amount of code, we have chosen to provide the Github link to the code repository. The code is available at \url{https://github.com/JohanHerbert/ThesisWork}. The code for the marginal model test is located in the \emph{MarginalExperiment.ipynb} file. The code for the hyperparameter test is located in the \emph{BigTestKebnekaise.py} file. Finally, the code for the copula test is located in the \emph{MainTestAllCopulas.ipynb} file.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Further Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Further Experiments}\label{sec:FurtherExperiments}
% If you have done extensive testing but do not want to include all of them in the main body of the thesis, then you can place them in the appendix and refer to them in the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Declarations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Confidentiality}
The authors have no relevant confidentiality agreements to disclose.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Declarations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Declarations}
The authors have no relevant financial or non-financial interests to disclose.
\end{document}

